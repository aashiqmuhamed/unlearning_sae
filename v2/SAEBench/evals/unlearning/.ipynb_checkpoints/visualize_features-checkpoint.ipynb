{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5326acc3-e750-42b5-88bd-a1f8f09f67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\"\n",
    "model_dir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "\n",
    "# finetuned_path points to the finetuned model\n",
    "finetuned_path = os.path.join(base_dir, \"alpha_0.1\", model_dir)\n",
    "# baseline_path points to the baseline model\n",
    "baseline_path = os.path.join(base_dir, \"base_model\", model_dir)\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load activation and MCQ results from a directory.\"\"\"\n",
    "    acts_forget = np.load(os.path.join(path, \"activations/feature_acts_forget.npy\"))\n",
    "    acts_retain = np.load(os.path.join(path, \"activations/feature_acts_retain.npy\"))\n",
    "    mcq_results = np.load(os.path.join(path, \"mcq_performance/mcq_results.npy\"), allow_pickle=True).item()\n",
    "    return acts_forget, acts_retain, mcq_results\n",
    "\n",
    "def plot_activation_distributions(finetuned_acts, baseline_acts, title, save_path):\n",
    "    \"\"\"Create histogram comparison of activations.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(baseline_acts, bins=50, alpha=0.5, label='Baseline', density=True)\n",
    "    plt.hist(finetuned_acts, bins=50, alpha=0.5, label='Finetuned', density=True)\n",
    "    plt.title(f'Distribution of {title}')\n",
    "    plt.xlabel('Activation Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_activation_scatter(finetuned_acts, baseline_acts, title, save_path):\n",
    "    \"\"\"Create scatter plot comparing activations between models.\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(baseline_acts, finetuned_acts, alpha=0.5)\n",
    "    plt.plot([min(baseline_acts.min(), finetuned_acts.min()), \n",
    "              max(baseline_acts.max(), finetuned_acts.max())], \n",
    "             [min(baseline_acts.min(), finetuned_acts.min()), \n",
    "              max(baseline_acts.max(), finetuned_acts.max())], \n",
    "             'r--', label='y=x')\n",
    "    plt.xlabel('Baseline Model Activations')\n",
    "    plt.ylabel('Finetuned Model Activations')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def compare_mcq_results(finetuned_results, baseline_results):\n",
    "    \"\"\"Compare MCQ metrics between models.\"\"\"\n",
    "    metrics_comparison = {}\n",
    "    \n",
    "    # Compare mean_correct for each dataset\n",
    "    for dataset in finetuned_results.keys():\n",
    "        if dataset in baseline_results:\n",
    "            metrics_comparison[dataset] = {\n",
    "                'finetuned_mean_correct': finetuned_results[dataset]['mean_correct'],\n",
    "                'baseline_mean_correct': baseline_results[dataset]['mean_correct'],\n",
    "                'difference': finetuned_results[dataset]['mean_correct'] - baseline_results[dataset]['mean_correct']\n",
    "            }\n",
    "    \n",
    "    return metrics_comparison\n",
    "\n",
    "def plot_mcq_comparison(metrics_comparison, save_path):\n",
    "    \"\"\"Create bar plot comparing MCQ performance.\"\"\"\n",
    "    datasets = list(metrics_comparison.keys())\n",
    "    finetuned_scores = [metrics_comparison[d]['finetuned_mean_correct'] for d in datasets]\n",
    "    baseline_scores = [metrics_comparison[d]['baseline_mean_correct'] for d in datasets]\n",
    "\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, baseline_scores, width, label='Baseline')\n",
    "    ax.bar(x + width/2, finetuned_scores, width, label='Finetuned')\n",
    "\n",
    "    ax.set_ylabel('Mean Correct')\n",
    "    ax.set_title('MCQ Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_activation_bars(finetuned_forget, baseline_forget, finetuned_retain, baseline_retain, save_path):\n",
    "    \"\"\"Create bar plot comparing mean activations between models.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(2)  # Two groups: Forget and Retain\n",
    "    width = 0.35\n",
    "    \n",
    "    # Calculate means\n",
    "    baseline_means = [baseline_forget.mean(), baseline_retain.mean()]\n",
    "    finetuned_means = [finetuned_forget.mean(), finetuned_retain.mean()]\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    baseline_sems = [baseline_forget.std() / np.sqrt(len(baseline_forget)), \n",
    "                    baseline_retain.std() / np.sqrt(len(baseline_retain))]\n",
    "    finetuned_sems = [finetuned_forget.std() / np.sqrt(len(finetuned_forget)), \n",
    "                      finetuned_retain.std() / np.sqrt(len(finetuned_retain))]\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(x - width/2, baseline_means, width, label='Baseline', yerr=baseline_sems, capsize=5)\n",
    "    plt.bar(x + width/2, finetuned_means, width, label='Finetuned', yerr=finetuned_sems, capsize=5)\n",
    "    \n",
    "    plt.ylabel('Mean Activation')\n",
    "    plt.title('Mean Activations Comparison')\n",
    "    plt.xticks(x, ['Forget', 'Retain'])\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(baseline_means):\n",
    "        plt.text(i - width/2, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(finetuned_means):\n",
    "        plt.text(i + width/2, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_activation_bars_normalized(finetuned_forget, baseline_forget, finetuned_retain, baseline_retain, save_path):\n",
    "    \"\"\"Create bar plot comparing normalized mean activations between models.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(2)  # Two groups: Forget and Retain\n",
    "    width = 0.35\n",
    "    \n",
    "    # Calculate means and normalize by baseline\n",
    "    baseline_means = [baseline_forget.mean(), baseline_retain.mean()]\n",
    "    finetuned_means = [finetuned_forget.mean(), finetuned_retain.mean()]\n",
    "    \n",
    "    # Normalize by baseline\n",
    "    normalized_finetuned = [fm/bm for fm, bm in zip(finetuned_means, baseline_means)]\n",
    "    normalized_baseline = [1.0, 1.0]  # Baseline normalized to 1\n",
    "    \n",
    "    # Calculate standard errors (normalized)\n",
    "    baseline_sems = [0, 0]  # No error bars for baseline since it's normalized to 1\n",
    "    finetuned_sems = [\n",
    "        (finetuned_forget.std() / np.sqrt(len(finetuned_forget))) / baseline_means[0],\n",
    "        (finetuned_retain.std() / np.sqrt(len(finetuned_retain))) / baseline_means[1]\n",
    "    ]\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(x - width/2, normalized_baseline, width, label='Baseline', yerr=baseline_sems, capsize=5)\n",
    "    plt.bar(x + width/2, normalized_finetuned, width, label='Finetuned', yerr=finetuned_sems, capsize=5)\n",
    "    \n",
    "    plt.ylabel('Normalized Mean Activation\\n(Relative to Baseline)')\n",
    "    plt.title('Normalized Mean Activations Comparison')\n",
    "    plt.xticks(x, ['Forget', 'Retain'])\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(normalized_baseline):\n",
    "        plt.text(i - width/2, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(normalized_finetuned):\n",
    "        plt.text(i + width/2, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_position_wise_activations(finetuned_forget, baseline_forget, finetuned_retain, baseline_retain, save_path, num_positions=200):\n",
    "    \"\"\"Create bar plot comparing activations at each position.\"\"\"\n",
    "    # Use only first num_positions for visibility\n",
    "    positions = np.arange(num_positions)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[1, 1])\n",
    "    width = 0.35\n",
    "    \n",
    "    # Forget activations plot\n",
    "    ax1.bar(positions - width/2, baseline_forget[:num_positions], width, label='Baseline', alpha=0.7)\n",
    "    ax1.bar(positions + width/2, finetuned_forget[:num_positions], width, label='Finetuned', alpha=0.7)\n",
    "    ax1.set_title('Forget Feature Activations by Position')\n",
    "    ax1.set_ylabel('Activation Value')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Retain activations plot\n",
    "    ax2.bar(positions - width/2, baseline_retain[:num_positions], width, label='Baseline', alpha=0.7)\n",
    "    ax2.bar(positions + width/2, finetuned_retain[:num_positions], width, label='Finetuned', alpha=0.7)\n",
    "    ax2.set_title('Retain Feature Activations by Position')\n",
    "    ax2.set_xlabel('Feature Position')\n",
    "    ax2.set_ylabel('Activation Value')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_position_wise_activations_diff(finetuned_forget, baseline_forget, finetuned_retain, baseline_retain, save_path, num_positions=200):\n",
    "    \"\"\"Create bar plot showing differences in activations at each position.\"\"\"\n",
    "    # Calculate differences (finetuned - baseline)\n",
    "    forget_diff = finetuned_forget[:num_positions] - baseline_forget[:num_positions]\n",
    "    retain_diff = finetuned_retain[:num_positions] - baseline_retain[:num_positions]\n",
    "    \n",
    "    positions = np.arange(num_positions)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[1, 1])\n",
    "    \n",
    "    # Forget activations difference plot\n",
    "    ax1.bar(positions, forget_diff, alpha=0.7)\n",
    "    ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    ax1.set_title('Difference in Forget Feature Activations (Finetuned - Baseline)')\n",
    "    ax1.set_ylabel('Activation Difference')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Retain activations difference plot\n",
    "    ax2.bar(positions, retain_diff, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    ax2.set_title('Difference in Retain Feature Activations (Finetuned - Baseline)')\n",
    "    ax2.set_xlabel('Feature Position')\n",
    "    ax2.set_ylabel('Activation Difference')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \"\"\"Create bar plot comparing MCQ performance.\"\"\"\n",
    "    datasets = list(metrics_comparison.keys())\n",
    "    finetuned_scores = [metrics_comparison[d]['finetuned_mean_correct'] for d in datasets]\n",
    "    baseline_scores = [metrics_comparison[d]['baseline_mean_correct'] for d in datasets]\n",
    "\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, baseline_scores, width, label='Baseline')\n",
    "    ax.bar(x + width/2, finetuned_scores, width, label='Finetuned')\n",
    "\n",
    "    ax.set_ylabel('Mean Correct')\n",
    "    ax.set_title('MCQ Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04c98563-8f02-46d3-8b0f-1816f6721447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from finetuned model...\n",
      "Loading data from baseline model...\n",
      "Plotting activation distributions...\n",
      "Plotting activation scatter plots...\n",
      "Plotting activation bar comparisons...\n",
      "Comparing MCQ results...\n",
      "\n",
      "MCQ Performance Comparison:\n",
      "\n",
      "Dataset: wmdp-bio\n",
      "Baseline model mean correct: 1.0000\n",
      "Finetuned model mean correct: 0.2295\n",
      "Difference (Finetuned - Baseline): -0.7705\n",
      "\n",
      "Dataset: high_school_us_history\n",
      "Baseline model mean correct: 1.0000\n",
      "Finetuned model mean correct: 0.2294\n",
      "Difference (Finetuned - Baseline): -0.7706\n",
      "\n",
      "Dataset: college_computer_science\n",
      "Baseline model mean correct: 1.0000\n",
      "Finetuned model mean correct: 0.4444\n",
      "Difference (Finetuned - Baseline): -0.5556\n",
      "\n",
      "Dataset: high_school_geography\n",
      "Baseline model mean correct: 1.0000\n",
      "Finetuned model mean correct: 0.2019\n",
      "Difference (Finetuned - Baseline): -0.7981\n",
      "\n",
      "Dataset: human_aging\n",
      "Baseline model mean correct: 1.0000\n",
      "Finetuned model mean correct: 0.3214\n",
      "Difference (Finetuned - Baseline): -0.6786\n",
      "Plotting position-wise activation comparisons...\n",
      "\n",
      "Activation Statistics:\n",
      "\n",
      "Forget Activations:\n",
      "Baseline  - Mean: 0.0236, Std: 0.1247\n",
      "Finetuned - Mean: 0.1077, Std: 1.6485\n",
      "\n",
      "Retain Activations:\n",
      "Baseline  - Mean: 0.0226, Std: 0.1204\n",
      "Finetuned - Mean: 0.0574, Std: 0.8174\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for plots\n",
    "output_dir = \"comparison_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load data from both models\n",
    "print(\"Loading data from finetuned model...\")\n",
    "finetuned_forget, finetuned_retain, finetuned_mcq = load_data(finetuned_path)\n",
    "print(\"Loading data from baseline model...\")\n",
    "baseline_forget, baseline_retain, baseline_mcq = load_data(baseline_path)\n",
    "\n",
    "# Plot activation distributions\n",
    "print(\"Plotting activation distributions...\")\n",
    "plot_activation_distributions(\n",
    "    finetuned_forget, baseline_forget,\n",
    "    \"Forget Activations\",\n",
    "    os.path.join(output_dir, \"forget_activations_dist.png\")\n",
    ")\n",
    "plot_activation_distributions(\n",
    "    finetuned_retain, baseline_retain,\n",
    "    \"Retain Activations\",\n",
    "    os.path.join(output_dir, \"retain_activations_dist.png\")\n",
    ")\n",
    "\n",
    "# Plot activation scatter plots\n",
    "print(\"Plotting activation scatter plots...\")\n",
    "plot_activation_scatter(\n",
    "    finetuned_forget, baseline_forget,\n",
    "    \"Forget Activations Comparison\",\n",
    "    os.path.join(output_dir, \"forget_activations_scatter.png\")\n",
    ")\n",
    "plot_activation_scatter(\n",
    "    finetuned_retain, baseline_retain,\n",
    "    \"Retain Activations Comparison\",\n",
    "    os.path.join(output_dir, \"retain_activations_scatter.png\")\n",
    ")\n",
    "\n",
    "# Plot activation bar comparisons\n",
    "print(\"Plotting activation bar comparisons...\")\n",
    "plot_activation_bars(\n",
    "    finetuned_forget, baseline_forget,\n",
    "    finetuned_retain, baseline_retain,\n",
    "    os.path.join(output_dir, \"activations_bars.png\")\n",
    ")\n",
    "plot_activation_bars_normalized(\n",
    "    finetuned_forget, baseline_forget,\n",
    "    finetuned_retain, baseline_retain,\n",
    "    os.path.join(output_dir, \"activations_bars_normalized.png\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Compare MCQ results\n",
    "print(\"Comparing MCQ results...\")\n",
    "metrics_comparison = compare_mcq_results(finetuned_mcq, baseline_mcq)\n",
    "\n",
    "# Print MCQ comparison\n",
    "print(\"\\nMCQ Performance Comparison:\")\n",
    "for dataset, metrics in metrics_comparison.items():\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    print(f\"Baseline model mean correct: {metrics['baseline_mean_correct']:.4f}\")\n",
    "    print(f\"Finetuned model mean correct: {metrics['finetuned_mean_correct']:.4f}\")\n",
    "    print(f\"Difference (Finetuned - Baseline): {metrics['difference']:.4f}\")\n",
    "\n",
    "# Plot MCQ comparison\n",
    "plot_mcq_comparison(\n",
    "    metrics_comparison,\n",
    "    os.path.join(output_dir, \"mcq_performance_comparison.png\")\n",
    ")\n",
    "\n",
    "# Plot position-wise activation comparisons\n",
    "print(\"Plotting position-wise activation comparisons...\")\n",
    "plot_position_wise_activations(\n",
    "    finetuned_forget, baseline_forget,\n",
    "    finetuned_retain, baseline_retain,\n",
    "    os.path.join(output_dir, \"position_wise_activations.png\")\n",
    ")\n",
    "plot_position_wise_activations_diff(\n",
    "    finetuned_forget, baseline_forget,\n",
    "    finetuned_retain, baseline_retain,\n",
    "    os.path.join(output_dir, \"position_wise_activations_diff.png\")\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate and print activation statistics\n",
    "print(\"\\nActivation Statistics:\")\n",
    "print(\"\\nForget Activations:\")\n",
    "print(f\"Baseline  - Mean: {baseline_forget.mean():.4f}, Std: {baseline_forget.std():.4f}\")\n",
    "print(f\"Finetuned - Mean: {finetuned_forget.mean():.4f}, Std: {finetuned_forget.std():.4f}\")\n",
    "\n",
    "print(\"\\nRetain Activations:\")\n",
    "print(f\"Baseline  - Mean: {baseline_retain.mean():.4f}, Std: {baseline_retain.std():.4f}\")\n",
    "print(f\"Finetuned - Mean: {finetuned_retain.mean():.4f}, Std: {finetuned_retain.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e40ab4-d3c0-4bba-ab81-92e0509be8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# def load_and_compare_activations():\n",
    "#     # Define paths\n",
    "#     base_dir = \"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\"\n",
    "#     model_subdir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "    \n",
    "#     # Load baseline data\n",
    "#     baseline_path = os.path.join(base_dir, \"base_model\", model_subdir)\n",
    "#     baseline_forget = np.load(os.path.join(baseline_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     baseline_retain = np.load(os.path.join(baseline_path, \"activations/feature_acts_retain.npy\"))\n",
    "#     baseline_mcq = np.load(os.path.join(baseline_path, \"mcq_performance/mcq_results.npy\"), allow_pickle=True).item()\n",
    "\n",
    "#     # Load finetuned data\n",
    "#     finetuned_path = os.path.join(base_dir, model_subdir)\n",
    "#     finetuned_forget = np.load(os.path.join(finetuned_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     finetuned_retain = np.load(os.path.join(finetuned_path, \"activations/feature_acts_retain.npy\"))\n",
    "#     finetuned_mcq = np.load(os.path.join(finetuned_path, \"mcq_performance/mcq_results.npy\"), allow_pickle=True).item()\n",
    "\n",
    "#     # Print shapes to verify\n",
    "#     print(\"Data shapes:\")\n",
    "#     print(f\"Baseline forget: {baseline_forget.shape}\")\n",
    "#     print(f\"Baseline retain: {baseline_retain.shape}\")\n",
    "#     print(f\"Finetuned forget: {finetuned_forget.shape}\")\n",
    "#     print(f\"Finetuned retain: {finetuned_retain.shape}\")\n",
    "\n",
    "#     # Create plot comparing activations\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "    \n",
    "#     # Number of features to show (first N features)\n",
    "#     n_features = 50  # Adjust this number as needed\n",
    "#     feature_indices = np.arange(n_features)\n",
    "    \n",
    "#     # Plot Forget Features\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     plt.bar(feature_indices - 0.2, baseline_forget[:n_features], width=0.4, label='Baseline', alpha=0.7)\n",
    "#     plt.bar(feature_indices + 0.2, finetuned_forget[:n_features], width=0.4, label='Finetuned', alpha=0.7)\n",
    "#     plt.title('Forget Features')\n",
    "#     plt.ylabel('Activation')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Plot Retain Features\n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     plt.bar(feature_indices - 0.2, baseline_retain[:n_features], width=0.4, label='Baseline', alpha=0.7)\n",
    "#     plt.bar(feature_indices + 0.2, finetuned_retain[:n_features], width=0.4, label='Finetuned', alpha=0.7)\n",
    "#     plt.title('Retain Features')\n",
    "#     plt.xlabel('Feature Index')\n",
    "#     plt.ylabel('Activation')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('activation_comparison.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Print some basic statistics\n",
    "#     print(\"\\nActivation Statistics:\")\n",
    "#     print(\"\\nForget Features:\")\n",
    "#     print(f\"Baseline  - Mean: {baseline_forget.mean():.4f}, Std: {baseline_forget.std():.4f}\")\n",
    "#     print(f\"Finetuned - Mean: {finetuned_forget.mean():.4f}, Std: {finetuned_forget.std():.4f}\")\n",
    "    \n",
    "#     print(\"\\nRetain Features:\")\n",
    "#     print(f\"Baseline  - Mean: {baseline_retain.mean():.4f}, Std: {baseline_retain.std():.4f}\")\n",
    "#     print(f\"Finetuned - Mean: {finetuned_retain.mean():.4f}, Std: {finetuned_retain.std():.4f}\")\n",
    "\n",
    "#     # Print MCQ performance\n",
    "#     print(\"\\nMCQ Performance:\")\n",
    "#     for dataset in baseline_mcq.keys():\n",
    "#         if dataset in finetuned_mcq:\n",
    "#             print(f\"\\nDataset: {dataset}\")\n",
    "#             print(f\"Baseline mean correct: {baseline_mcq[dataset]['mean_correct']:.4f}\")\n",
    "#             print(f\"Finetuned mean correct: {finetuned_mcq[dataset]['mean_correct']:.4f}\")\n",
    "\n",
    "\n",
    "# load_and_compare_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31be8b2b-b1cd-4f72-baac-9618e53a15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# def load_and_plot_sorted_activations():\n",
    "#     # Define paths\n",
    "#     base_dir = \"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\"\n",
    "#     model_subdir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "    \n",
    "#     # Load baseline data\n",
    "#     baseline_path = os.path.join(base_dir, \"base_model\", model_subdir)\n",
    "#     baseline_forget = np.load(os.path.join(baseline_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     baseline_retain = np.load(os.path.join(baseline_path, \"activations/feature_acts_retain.npy\"))\n",
    "\n",
    "#     # Load finetuned data\n",
    "#     finetuned_path = os.path.join(base_dir, model_subdir)\n",
    "#     finetuned_forget = np.load(os.path.join(finetuned_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     finetuned_retain = np.load(os.path.join(finetuned_path, \"activations/feature_acts_retain.npy\"))\n",
    "\n",
    "#     # Print shapes to verify\n",
    "#     print(\"Data shapes:\")\n",
    "#     print(f\"Baseline forget: {baseline_forget.shape}\")\n",
    "#     print(f\"Baseline retain: {baseline_retain.shape}\")\n",
    "#     print(f\"Finetuned forget: {finetuned_forget.shape}\")\n",
    "#     print(f\"Finetuned retain: {finetuned_retain.shape}\")\n",
    "\n",
    "#     # Get sorting indices based on finetuned retain activations\n",
    "#     sort_indices = np.argsort(-finetuned_retain)  # negative for descending order\n",
    "    \n",
    "#     # Sort all arrays using these indices\n",
    "#     baseline_forget_sorted = baseline_forget[sort_indices]\n",
    "#     baseline_retain_sorted = baseline_retain[sort_indices]\n",
    "#     finetuned_forget_sorted = finetuned_forget[sort_indices]\n",
    "#     finetuned_retain_sorted = finetuned_retain[sort_indices]\n",
    "\n",
    "#     # Create plot\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     feature_indices = np.arange(len(baseline_forget))\n",
    "    \n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     plt.bar(feature_indices - 0.2, baseline_forget_sorted, width=0.4, label='Baseline', alpha=0.7)\n",
    "#     plt.bar(feature_indices + 0.2, finetuned_forget_sorted, width=0.4, label='Finetuned', alpha=0.7)\n",
    "#     plt.title('Forget Features (Sorted by Retain Feature Activations)')\n",
    "#     plt.ylabel('Forget Activation')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     plt.bar(feature_indices - 0.2, baseline_retain_sorted, width=0.4, label='Baseline', alpha=0.7)\n",
    "#     plt.bar(feature_indices + 0.2, finetuned_retain_sorted, width=0.4, label='Finetuned', alpha=0.7)\n",
    "#     plt.title('Retain Features (Sorted by Activation Value)')\n",
    "#     plt.xlabel('Feature Index (Sorted)')\n",
    "#     plt.ylabel('Retain Activation')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('activation_comparison_sorted.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Print statistics about sorted activations\n",
    "#     print(\"\\nActivation Statistics (Top 10 features by retain activation):\")\n",
    "#     print(\"\\nForget Features:\")\n",
    "#     print(\"Baseline  - Mean of top 10:\", baseline_forget_sorted[:10].mean())\n",
    "#     print(\"Finetuned - Mean of top 10:\", finetuned_forget_sorted[:10].mean())\n",
    "    \n",
    "#     print(\"\\nRetain Features:\")\n",
    "#     print(\"Baseline  - Mean of top 10:\", baseline_retain_sorted[:10].mean())\n",
    "#     print(\"Finetuned - Mean of top 10:\", finetuned_retain_sorted[:10].mean())\n",
    "\n",
    "#     # Print correlation between forget and retain features\n",
    "#     baseline_corr = np.corrcoef(baseline_forget_sorted, baseline_retain_sorted)[0,1]\n",
    "#     finetuned_corr = np.corrcoef(finetuned_forget_sorted, finetuned_retain_sorted)[0,1]\n",
    "    \n",
    "#     print(\"\\nCorrelations between forget and retain features:\")\n",
    "#     print(f\"Baseline: {baseline_corr:.4f}\")\n",
    "#     print(f\"Finetuned: {finetuned_corr:.4f}\")\n",
    "\n",
    "\n",
    "# load_and_plot_sorted_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaba558-18ae-40c3-ac79-ad69fc732a42",
   "metadata": {},
   "source": [
    "# Log plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163785e0-1cd4-443a-870d-e60eb4ca24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "Baseline forget: (16384,)\n",
      "Baseline retain: (16384,)\n",
      "Finetuned forget: (16384,)\n",
      "Finetuned retain: (16384,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_387168/2053434577.py:61: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_387168/2053434577.py:62: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig('activation_comparison_sorted.png')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activation Statistics (Top 10 features by retain activation):\n",
      "\n",
      "Forget Features:\n",
      "Baseline  - Mean of top 10: 4.043584\n",
      "Finetuned - Mean of top 10: 6.516452\n",
      "\n",
      "Retain Features:\n",
      "Baseline  - Mean of top 10: 4.0422974\n",
      "Finetuned - Mean of top 10: 8.988276\n",
      "\n",
      "Correlations between forget and retain features:\n",
      "Baseline: 0.9287\n",
      "Finetuned: 0.0880\n"
     ]
    }
   ],
   "source": [
    "# Log plot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def load_and_plot_sorted_activations():\n",
    "    # Define paths\n",
    "    base_dir = \"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\"\n",
    "    model_subdir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "    \n",
    "    # Load baseline data\n",
    "    baseline_path = os.path.join(base_dir, \"base_model\", model_subdir)\n",
    "    baseline_forget = np.load(os.path.join(baseline_path, \"activations/feature_acts_forget.npy\"))\n",
    "    baseline_retain = np.load(os.path.join(baseline_path, \"activations/feature_acts_retain.npy\"))\n",
    "\n",
    "    # Load finetuned data\n",
    "    finetuned_path = os.path.join(base_dir, \"alpha_0.1\", model_subdir)\n",
    "    finetuned_forget = np.load(os.path.join(finetuned_path, \"activations/feature_acts_forget.npy\"))\n",
    "    finetuned_retain = np.load(os.path.join(finetuned_path, \"activations/feature_acts_retain.npy\"))\n",
    "\n",
    "    # Print shapes to verify\n",
    "    print(\"Data shapes:\")\n",
    "    print(f\"Baseline forget: {baseline_forget.shape}\")\n",
    "    print(f\"Baseline retain: {baseline_retain.shape}\")\n",
    "    print(f\"Finetuned forget: {finetuned_forget.shape}\")\n",
    "    print(f\"Finetuned retain: {finetuned_retain.shape}\")\n",
    "\n",
    "    # Get sorting indices based on finetuned retain activations\n",
    "    sort_indices = np.argsort(-baseline_retain)  # negative for descending order\n",
    "    \n",
    "    # Sort all arrays using these indices\n",
    "    baseline_forget_sorted = baseline_forget[sort_indices]\n",
    "    baseline_retain_sorted = baseline_retain[sort_indices]\n",
    "    finetuned_forget_sorted = finetuned_forget[sort_indices]\n",
    "    finetuned_retain_sorted = finetuned_retain[sort_indices]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    feature_indices = np.arange(len(baseline_forget))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(feature_indices - 0.2, np.abs(baseline_forget_sorted), width=0.4, label='Baseline', alpha=0.7)\n",
    "    plt.bar(feature_indices + 0.2, np.abs(finetuned_forget_sorted), width=0.4, label='Finetuned', alpha=0.7)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Forget Features (Sorted by Retain Feature Activations)')\n",
    "    plt.ylabel('Forget Activation (log scale)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, which=\"both\")\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(feature_indices - 0.2, np.abs(baseline_retain_sorted), width=0.4, label='Baseline', alpha=0.7)\n",
    "    plt.bar(feature_indices + 0.2, np.abs(finetuned_retain_sorted), width=0.4, label='Finetuned', alpha=0.7)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Retain Features (Sorted by Activation Value)')\n",
    "    plt.xlabel('Feature Index (Sorted)')\n",
    "    plt.ylabel('Retain Activation (log scale)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, which=\"both\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('activation_comparison_sorted.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Print statistics about sorted activations\n",
    "    print(\"\\nActivation Statistics (Top 10 features by retain activation):\")\n",
    "    print(\"\\nForget Features:\")\n",
    "    print(\"Baseline  - Mean of top 10:\", baseline_forget_sorted[:10].mean())\n",
    "    print(\"Finetuned - Mean of top 10:\", finetuned_forget_sorted[:10].mean())\n",
    "    \n",
    "    print(\"\\nRetain Features:\")\n",
    "    print(\"Baseline  - Mean of top 10:\", baseline_retain_sorted[:10].mean())\n",
    "    print(\"Finetuned - Mean of top 10:\", finetuned_retain_sorted[:10].mean())\n",
    "\n",
    "    # Print correlation between forget and retain features\n",
    "    baseline_corr = np.corrcoef(baseline_forget_sorted, baseline_retain_sorted)[0,1]\n",
    "    finetuned_corr = np.corrcoef(finetuned_forget_sorted, finetuned_retain_sorted)[0,1]\n",
    "    \n",
    "    print(\"\\nCorrelations between forget and retain features:\")\n",
    "    print(f\"Baseline: {baseline_corr:.4f}\")\n",
    "    print(f\"Finetuned: {finetuned_corr:.4f}\")\n",
    "\n",
    "\n",
    "load_and_plot_sorted_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d79da-e1fd-45e1-b949-cb1973ec0a9a",
   "metadata": {},
   "source": [
    "# Revised shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311b8724-30b6-4eaf-a459-43bbce99f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "\n",
    "# def plot_aggregated_activations(baseline_forget, baseline_retain, \n",
    "#                               finetuned_forget, finetuned_retain):\n",
    "#     # Create figure with subplots\n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "#     # 1. Feature activation changes (log scale)\n",
    "#     ax1 = plt.subplot(221)\n",
    "#     sort_indices = np.argsort(-baseline_retain)\n",
    "#     x = np.arange(len(baseline_retain))\n",
    "    \n",
    "#     # Change alpha values in plot 1\n",
    "#     ax1.scatter(x, np.abs(baseline_retain[sort_indices]), alpha=0.3, label='Baseline Retain', color='blue', s=10)\n",
    "#     ax1.scatter(x, np.abs(finetuned_retain[sort_indices]), alpha=0.3, label='Finetuned Retain', color='lightblue', s=10)\n",
    "#     ax1.scatter(x, np.abs(baseline_forget[sort_indices]), alpha=0.3, label='Baseline Forget', color='red', s=10)\n",
    "#     ax1.scatter(x, np.abs(finetuned_forget[sort_indices]), alpha=0.3, label='Finetuned Forget', color='lightcoral', s=10)\n",
    "    \n",
    "#     ax1.set_yscale('log')\n",
    "#     ax1.set_xlabel('Feature Index (sorted by baseline retain activation)')\n",
    "#     ax1.set_ylabel('Activation Magnitude (log scale)')\n",
    "#     ax1.set_title('Feature Activations (Log Scale)')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "    \n",
    "    \n",
    "#     # 2. Selectivity analysis (log scale for y)\n",
    "#     ax2 = plt.subplot(222)\n",
    "#     baseline_selectivity = np.abs(baseline_retain) / (np.abs(baseline_forget) + 1e-10)\n",
    "#     finetuned_selectivity = np.abs(finetuned_retain) / (np.abs(finetuned_forget) + 1e-10)\n",
    "    \n",
    "#     # Sort by baseline selectivity\n",
    "#     select_sort = np.argsort(-baseline_selectivity)\n",
    "\n",
    "#     ax2.plot(baseline_selectivity[select_sort], label='Baseline', alpha=0.4)\n",
    "#     ax2.plot(finetuned_selectivity[select_sort], label='Finetuned', alpha=0.4)\n",
    "\n",
    "#     # ax2.plot(baseline_selectivity[select_sort], label='Baseline', alpha=0.7)\n",
    "#     # ax2.plot(finetuned_selectivity[select_sort], label='Finetuned', alpha=0.7)\n",
    "    \n",
    "#     ax2.set_yscale('log')\n",
    "#     ax2.set_xlabel('Feature Index (sorted by baseline selectivity)')\n",
    "#     ax2.set_ylabel('Selectivity |retain|/|forget| (log scale)')\n",
    "#     ax2.set_title('Feature Selectivity Comparison')\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "#     # 3. Joint distribution (log scale)\n",
    "#     ax3 = plt.subplot(223)\n",
    "#     retain_mask = baseline_retain > np.median(baseline_retain)\n",
    "    \n",
    "#     # Create scatter plot with different colors for high/low retain features\n",
    "#     # ax3.scatter(np.abs(baseline_forget[~retain_mask]), np.abs(finetuned_forget[~retain_mask]), \n",
    "#     #             alpha=0.5, label='Low Retain', color='gray', s=10)\n",
    "#     # ax3.scatter(np.abs(baseline_forget[retain_mask]), np.abs(finetuned_forget[retain_mask]), \n",
    "#     #             alpha=0.5, label='High Retain', color='red', s=10)\n",
    "\n",
    "#     ax3.scatter(np.abs(baseline_forget[~retain_mask]), np.abs(finetuned_forget[~retain_mask]), \n",
    "#             alpha=0.3, label='Low Retain', color='gray', s=10)\n",
    "#     ax3.scatter(np.abs(baseline_forget[retain_mask]), np.abs(finetuned_forget[retain_mask]), \n",
    "#             alpha=0.3, label='High Retain', color='red', s=10)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     max_val = max(np.max(np.abs(baseline_forget)), np.max(np.abs(finetuned_forget)))\n",
    "#     min_val = min(np.min(np.abs(baseline_forget[baseline_forget != 0])), \n",
    "#                  np.min(np.abs(finetuned_forget[finetuned_forget != 0])))\n",
    "    \n",
    "#     ax3.plot([min_val, max_val], [min_val, max_val], '--', color='gray', alpha=0.5)\n",
    "#     ax3.set_xscale('log')\n",
    "#     ax3.set_yscale('log')\n",
    "#     ax3.set_xlabel('Baseline Forget Activation (log scale)')\n",
    "#     ax3.set_ylabel('Finetuned Forget Activation (log scale)')\n",
    "#     ax3.set_title('Forget Activation Changes\\n(colored by baseline retain strength)')\n",
    "#     ax3.legend()\n",
    "#     ax3.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "#     # 4. Change ratio analysis\n",
    "#     ax4 = plt.subplot(224)\n",
    "#     change_ratio_retain = np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10)\n",
    "#     change_ratio_forget = np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)\n",
    "    \n",
    "#     # Sort by baseline retain activation\n",
    "#     # ax4.scatter(x, change_ratio_retain[sort_indices], alpha=0.5, label='Retain', color='blue', s=10)\n",
    "#     # ax4.scatter(x, change_ratio_forget[sort_indices], alpha=0.5, label='Forget', color='red', s=10)\n",
    "\n",
    "#     ax4.scatter(x, change_ratio_retain[sort_indices], alpha=0.3, label='Retain', color='blue', s=10)\n",
    "#     ax4.scatter(x, change_ratio_forget[sort_indices], alpha=0.3, label='Forget', color='red', s=10)\n",
    "    \n",
    "#     ax4.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "#     ax4.set_yscale('log')\n",
    "#     ax4.set_xlabel('Feature Index (sorted by baseline retain activation)')\n",
    "#     ax4.set_ylabel('Change Ratio (finetuned/baseline) (log scale)')\n",
    "#     ax4.set_title('Activation Change Ratios')\n",
    "#     ax4.legend()\n",
    "#     ax4.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     return fig\n",
    "\n",
    "# def print_activation_summary(baseline_forget, baseline_retain, \n",
    "#                            finetuned_forget, finetuned_retain):\n",
    "#     # Calculate key metrics using log space\n",
    "#     baseline_selectivity = np.abs(baseline_retain) / (np.abs(baseline_forget) + 1e-10)\n",
    "#     finetuned_selectivity = np.abs(finetuned_retain) / (np.abs(finetuned_forget) + 1e-10)\n",
    "    \n",
    "#     change_ratio_retain = np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10)\n",
    "#     change_ratio_forget = np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)\n",
    "    \n",
    "#     print(\"\\nActivation Analysis Summary:\")\n",
    "#     print(f\"\\nMedian Selectivity (|retain|/|forget|):\")\n",
    "#     print(f\"Baseline: {np.median(baseline_selectivity):.4f}\")\n",
    "#     print(f\"Finetuned: {np.median(finetuned_selectivity):.4f}\")\n",
    "    \n",
    "#     print(f\"\\nMedian Change Ratios (finetuned/baseline):\")\n",
    "#     print(f\"Retain: {np.median(change_ratio_retain):.4f}\")\n",
    "#     print(f\"Forget: {np.median(change_ratio_forget):.4f}\")\n",
    "    \n",
    "#     # Calculate percentage of features with improved metrics\n",
    "#     selectivity_improved = np.mean(finetuned_selectivity > baseline_selectivity) * 100\n",
    "#     retain_preserved = np.mean(change_ratio_retain > 0.9) * 100\n",
    "#     forget_reduced = np.mean(change_ratio_forget < 0.9) * 100\n",
    "    \n",
    "#     print(f\"\\nPercentage of features with:\")\n",
    "#     print(f\"Improved selectivity: {selectivity_improved:.1f}%\")\n",
    "#     print(f\"Preserved retain activation (>90%): {retain_preserved:.1f}%\")\n",
    "#     print(f\"Reduced forget activation (<90%): {forget_reduced:.1f}%\")\n",
    "\n",
    "# def main():\n",
    "#     # Define paths\n",
    "#     base_dir = \"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\"\n",
    "#     model_subdir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "    \n",
    "#     # Load baseline data\n",
    "#     baseline_path = os.path.join(base_dir, \"base_model\", model_subdir)\n",
    "#     baseline_forget = np.load(os.path.join(baseline_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     baseline_retain = np.load(os.path.join(baseline_path, \"activations/feature_acts_retain.npy\"))\n",
    "    \n",
    "#     # Load finetuned data\n",
    "#     finetuned_path = os.path.join(base_dir, \"alpha_0.1\", model_subdir)\n",
    "#     finetuned_forget = np.load(os.path.join(finetuned_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     finetuned_retain = np.load(os.path.join(finetuned_path, \"activations/feature_acts_retain.npy\"))\n",
    "    \n",
    "#     # Print data shapes\n",
    "#     print(\"Data shapes:\")\n",
    "#     print(f\"Baseline forget: {baseline_forget.shape}\")\n",
    "#     print(f\"Baseline retain: {baseline_retain.shape}\")\n",
    "#     print(f\"Finetuned forget: {finetuned_forget.shape}\")\n",
    "#     print(f\"Finetuned retain: {finetuned_retain.shape}\")\n",
    "    \n",
    "#     # Generate visualizations\n",
    "#     fig = plot_aggregated_activations(baseline_forget, baseline_retain,\n",
    "#                                     finetuned_forget, finetuned_retain)\n",
    "    \n",
    "#     # Save the figure\n",
    "#     fig.savefig('activation_analysis_log.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    \n",
    "#     # Print summary statistics\n",
    "#     print_activation_summary(baseline_forget, baseline_retain,\n",
    "#                            finetuned_forget, finetuned_retain)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d21f414f-f8a9-4101-9520-80da5e0299af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "\n",
    "# def plot_aggregated_activations(baseline_forget, baseline_retain, \n",
    "#                               finetuned_forget, finetuned_retain):\n",
    "#     \"\"\"\n",
    "#     Create a figure with four subplots analyzing different aspects of the activation patterns.\n",
    "#     \"\"\"\n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "#     # 1. Feature activation changes (log scale)\n",
    "#     ax1 = plt.subplot(221)\n",
    "#     sort_indices = np.argsort(-baseline_retain)\n",
    "#     x = np.arange(len(baseline_retain))\n",
    "    \n",
    "#     # Plot retains\n",
    "#     ax1.scatter(x, np.abs(baseline_retain[sort_indices]), alpha=0.15, \n",
    "#                 label='Baseline Retain', color='darkblue', s=5)\n",
    "#     ax1.scatter(x, np.abs(finetuned_retain[sort_indices]), alpha=0.15, \n",
    "#                 label='Finetuned Retain', color='blue', s=5)\n",
    "    \n",
    "#     # Plot forgets\n",
    "#     ax1.scatter(x, np.abs(baseline_forget[sort_indices]), alpha=0.15, \n",
    "#                 label='Baseline Forget', color='darkred', s=5)\n",
    "#     ax1.scatter(x, np.abs(finetuned_forget[sort_indices]), alpha=0.15, \n",
    "#                 label='Finetuned Forget', color='red', s=5)\n",
    "    \n",
    "#     ax1.set_yscale('log')\n",
    "#     ax1.set_xlabel('Feature Index (sorted by baseline retain activation)')\n",
    "#     ax1.set_ylabel('Activation Magnitude (log scale)')\n",
    "#     ax1.set_title('Feature Activations')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True, which=\"both\", ls=\"-\", alpha=0.1)\n",
    "    \n",
    "#     # 2. Selectivity analysis (log scale)\n",
    "#     ax2 = plt.subplot(222)\n",
    "#     baseline_selectivity = np.abs(baseline_retain) / (np.abs(baseline_forget) + 1e-10)\n",
    "#     finetuned_selectivity = np.abs(finetuned_retain) / (np.abs(finetuned_forget) + 1e-10)\n",
    "    \n",
    "#     select_sort = np.argsort(-baseline_selectivity)\n",
    "    \n",
    "#     # Plot selectivity curves\n",
    "#     ax2.plot(baseline_selectivity[select_sort], color='darkblue', \n",
    "#              label='Baseline', alpha=0.6, linewidth=1)\n",
    "#     ax2.plot(finetuned_selectivity[select_sort], color='red',\n",
    "#              label='Finetuned', alpha=0.6, linewidth=1)\n",
    "    \n",
    "#     ax2.set_yscale('log')\n",
    "#     ax2.set_xlabel('Feature Index (sorted by baseline selectivity)')\n",
    "#     ax2.set_ylabel('Selectivity |retain|/|forget| (log scale)')\n",
    "#     ax2.set_title('Feature Selectivity')\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True, which=\"both\", ls=\"-\", alpha=0.1)\n",
    "    \n",
    "#     # 3. Distribution of forget activations\n",
    "#     ax3 = plt.subplot(223)\n",
    "    \n",
    "#     # Use hexbin for better density visualization\n",
    "#     hb = ax3.hexbin(np.abs(baseline_forget), np.abs(finetuned_forget), \n",
    "#                     gridsize=30, bins='log', cmap='YlOrRd')\n",
    "#     plt.colorbar(hb, ax=ax3, label='Count (log scale)')\n",
    "    \n",
    "#     # Add reference line\n",
    "#     max_val = max(np.max(np.abs(baseline_forget)), np.max(np.abs(finetuned_forget)))\n",
    "#     min_val = min(np.min(np.abs(baseline_forget[baseline_forget != 0])), \n",
    "#                  np.min(np.abs(finetuned_forget[finetuned_forget != 0])))\n",
    "#     ax3.plot([min_val, max_val], [min_val, max_val], '--', color='gray', alpha=0.5)\n",
    "    \n",
    "#     ax3.set_xscale('log')\n",
    "#     ax3.set_yscale('log')\n",
    "#     ax3.set_xlabel('Baseline Forget Activation (log scale)')\n",
    "#     ax3.set_ylabel('Finetuned Forget Activation (log scale)')\n",
    "#     ax3.set_title('Forget Activation Changes\\n(density plot)')\n",
    "#     ax3.grid(True, which=\"both\", ls=\"-\", alpha=0.1)\n",
    "    \n",
    "#     # 4. Change ratio analysis\n",
    "#     ax4 = plt.subplot(224)\n",
    "#     change_ratio_retain = np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10)\n",
    "#     change_ratio_forget = np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)\n",
    "    \n",
    "#     # Calculate moving averages\n",
    "#     window = 500\n",
    "#     retain_ma = np.convolve(change_ratio_retain[sort_indices], \n",
    "#                            np.ones(window)/window, mode='valid')\n",
    "#     forget_ma = np.convolve(change_ratio_forget[sort_indices], \n",
    "#                            np.ones(window)/window, mode='valid')\n",
    "#     x_ma = np.arange(len(retain_ma))\n",
    "    \n",
    "#     # Plot moving averages and raw data\n",
    "#     ax4.plot(x_ma, retain_ma, color='blue', label='Retain (moving avg)', \n",
    "#              alpha=0.8, linewidth=2)\n",
    "#     ax4.plot(x_ma, forget_ma, color='red', label='Forget (moving avg)', \n",
    "#              alpha=0.8, linewidth=2)\n",
    "    \n",
    "#     ax4.scatter(x, change_ratio_retain[sort_indices], alpha=0.05, \n",
    "#                 color='blue', s=1)\n",
    "#     ax4.scatter(x, change_ratio_forget[sort_indices], alpha=0.05, \n",
    "#                 color='red', s=1)\n",
    "    \n",
    "#     ax4.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "#     ax4.set_yscale('log')\n",
    "#     ax4.set_xlabel('Feature Index (sorted by baseline retain activation)')\n",
    "#     ax4.set_ylabel('Change Ratio (finetuned/baseline) (log scale)')\n",
    "#     ax4.set_title('Activation Change Ratios')\n",
    "#     ax4.legend()\n",
    "#     ax4.grid(True, which=\"both\", ls=\"-\", alpha=0.1)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     return fig\n",
    "\n",
    "# def print_activation_summary(baseline_forget, baseline_retain, \n",
    "#                            finetuned_forget, finetuned_retain):\n",
    "#     \"\"\"\n",
    "#     Calculate and print summary statistics for the activation analysis.\n",
    "#     \"\"\"\n",
    "#     # Calculate selectivity metrics\n",
    "#     baseline_selectivity = np.abs(baseline_retain) / (np.abs(baseline_forget) + 1e-10)\n",
    "#     finetuned_selectivity = np.abs(finetuned_retain) / (np.abs(finetuned_forget) + 1e-10)\n",
    "    \n",
    "#     change_ratio_retain = np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10)\n",
    "#     change_ratio_forget = np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)\n",
    "    \n",
    "#     print(\"\\nActivation Analysis Summary:\")\n",
    "#     print(f\"\\nMedian Selectivity (|retain|/|forget|):\")\n",
    "#     print(f\"Baseline: {np.median(baseline_selectivity):.4f}\")\n",
    "#     print(f\"Finetuned: {np.median(finetuned_selectivity):.4f}\")\n",
    "    \n",
    "#     print(f\"\\nMedian Change Ratios (finetuned/baseline):\")\n",
    "#     print(f\"Retain: {np.median(change_ratio_retain):.4f}\")\n",
    "#     print(f\"Forget: {np.median(change_ratio_forget):.4f}\")\n",
    "    \n",
    "#     # Calculate improvement metrics\n",
    "#     selectivity_improved = np.mean(finetuned_selectivity > baseline_selectivity) * 100\n",
    "#     retain_preserved = np.mean(change_ratio_retain > 0.9) * 100\n",
    "#     forget_reduced = np.mean(change_ratio_forget < 0.9) * 100\n",
    "    \n",
    "#     print(f\"\\nPercentage of features with:\")\n",
    "#     print(f\"Improved selectivity: {selectivity_improved:.1f}%\")\n",
    "#     print(f\"Preserved retain activation (>90%): {retain_preserved:.1f}%\")\n",
    "#     print(f\"Reduced forget activation (<90%): {forget_reduced:.1f}%\")\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main function to load data and generate visualizations.\n",
    "#     \"\"\"\n",
    "#     # Define paths\n",
    "#     base_dir = \"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\"\n",
    "#     model_subdir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "    \n",
    "#     # Load baseline data\n",
    "#     baseline_path = os.path.join(base_dir, \"base_model\", model_subdir)\n",
    "#     baseline_forget = np.load(os.path.join(baseline_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     baseline_retain = np.load(os.path.join(baseline_path, \"activations/feature_acts_retain.npy\"))\n",
    "    \n",
    "#     # Load finetuned data\n",
    "#     finetuned_path = os.path.join(base_dir, \"alpha_0.1\", model_subdir)\n",
    "#     finetuned_forget = np.load(os.path.join(finetuned_path, \"activations/feature_acts_forget.npy\"))\n",
    "#     finetuned_retain = np.load(os.path.join(finetuned_path, \"activations/feature_acts_retain.npy\"))\n",
    "    \n",
    "#     # Print data shapes\n",
    "#     print(\"Data shapes:\")\n",
    "#     print(f\"Baseline forget: {baseline_forget.shape}\")\n",
    "#     print(f\"Baseline retain: {baseline_retain.shape}\")\n",
    "#     print(f\"Finetuned forget: {finetuned_forget.shape}\")\n",
    "#     print(f\"Finetuned retain: {finetuned_retain.shape}\")\n",
    "    \n",
    "#     # Generate visualizations\n",
    "#     fig = plot_aggregated_activations(baseline_forget, baseline_retain,\n",
    "#                                     finetuned_forget, finetuned_retain)\n",
    "    \n",
    "#     # Save the figure\n",
    "#     fig.savefig('activation_analysis_log.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    \n",
    "#     # Print summary statistics\n",
    "#     print_activation_summary(baseline_forget, baseline_retain,\n",
    "#                            finetuned_forget, finetuned_retain)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6395ee2c-8dc9-4764-a3a2-d0096b3e1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import LogNorm  # Added this import\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# def plot_improved_activations(baseline_forget, baseline_retain, \n",
    "#                             finetuned_forget, finetuned_retain):\n",
    "#     \"\"\"\n",
    "#     Create improved visualizations of activation patterns with better statistical measures\n",
    "#     and clearer density plotting.\n",
    "#     \"\"\"\n",
    "#     fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "#     # 1. Feature activation changes (log scale)\n",
    "#     ax1 = plt.subplot(221)\n",
    "#     sort_indices = np.argsort(-baseline_retain)\n",
    "#     x = np.arange(len(baseline_retain))\n",
    "    \n",
    "#     # Plot with reduced alpha and increased size for better visibility\n",
    "#     ax1.scatter(x, np.abs(baseline_retain[sort_indices]), alpha=0.2, \n",
    "#                 label='Baseline Retain', color='darkblue', s=10)\n",
    "#     ax1.scatter(x, np.abs(finetuned_retain[sort_indices]), alpha=0.2, \n",
    "#                 label='Finetuned Retain', color='blue', s=10)\n",
    "#     ax1.scatter(x, np.abs(baseline_forget[sort_indices]), alpha=0.2, \n",
    "#                 label='Baseline Forget', color='darkred', s=10)\n",
    "#     ax1.scatter(x, np.abs(finetuned_forget[sort_indices]), alpha=0.2, \n",
    "#                 label='Finetuned Forget', color='red', s=10)\n",
    "    \n",
    "#     ax1.set_yscale('log')\n",
    "#     ax1.set_xlabel('Feature Index (sorted by baseline retain activation)')\n",
    "#     ax1.set_ylabel('Activation Magnitude (log scale)')\n",
    "#     ax1.set_title('Feature Activations')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "#     # 2. Selectivity analysis with percentile bands\n",
    "#     ax2 = plt.subplot(222)\n",
    "#     baseline_selectivity = np.abs(baseline_retain) / (np.abs(baseline_forget) + 1e-10)\n",
    "#     finetuned_selectivity = np.abs(finetuned_retain) / (np.abs(finetuned_forget) + 1e-10)\n",
    "    \n",
    "#     select_sort = np.argsort(-baseline_selectivity)\n",
    "    \n",
    "#     # Calculate percentile bands\n",
    "#     def rolling_percentile(data, window=500):\n",
    "#         result = np.zeros_like(data)\n",
    "#         for i in range(len(data)):\n",
    "#             start_idx = max(0, i - window//2)\n",
    "#             end_idx = min(len(data), i + window//2)\n",
    "#             result[i] = np.median(data[start_idx:end_idx])\n",
    "#         return result\n",
    "    \n",
    "#     baseline_median = rolling_percentile(baseline_selectivity[select_sort])\n",
    "#     finetuned_median = rolling_percentile(finetuned_selectivity[select_sort])\n",
    "    \n",
    "#     ax2.plot(baseline_median, color='darkblue', label='Baseline (median)', linewidth=2)\n",
    "#     ax2.plot(finetuned_median, color='red', label='Finetuned (median)', linewidth=2)\n",
    "    \n",
    "#     ax2.set_yscale('log')\n",
    "#     ax2.set_xlabel('Feature Index (sorted by baseline selectivity)')\n",
    "#     ax2.set_ylabel('Selectivity |retain|/|forget| (log scale)')\n",
    "#     ax2.set_title('Feature Selectivity (with rolling median)')\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "#     # 3. Improved density plot of forget activations\n",
    "#     ax3 = plt.subplot(223)\n",
    "    \n",
    "#     # Create 2D histogram with more bins and better color scaling\n",
    "#     valid_mask = (baseline_forget != 0) & (finetuned_forget != 0)\n",
    "#     h, xedges, yedges = np.histogram2d(\n",
    "#         np.log10(np.abs(baseline_forget[valid_mask]) + 1e-10),\n",
    "#         np.log10(np.abs(finetuned_forget[valid_mask]) + 1e-10),\n",
    "#         bins=50,\n",
    "#         range=[[-6, 2], [-6, 2]]  # Set fixed range for better visualization\n",
    "#     )\n",
    "    \n",
    "#     # Plot as a heatmap with improved visibility\n",
    "#     pcm = ax3.pcolormesh(xedges, yedges, h.T, \n",
    "#                          norm=LogNorm(vmin=1, vmax=h.max()),\n",
    "#                          cmap='viridis')\n",
    "#     plt.colorbar(pcm, ax=ax3, label='Count (log scale)')\n",
    "    \n",
    "#     # Add reference line\n",
    "#     ax3.plot([-6, 2], [-6, 2], '--', color='red', alpha=0.8, label='y=x')\n",
    "    \n",
    "#     ax3.set_xlabel('Baseline Forget Activation (log10 scale)')\n",
    "#     ax3.set_ylabel('Finetuned Forget Activation (log10 scale)')\n",
    "#     ax3.set_title('Forget Activation Changes\\n(improved density visualization)')\n",
    "#     ax3.legend()\n",
    "#     ax3.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "#     # 4. Change ratio analysis with quantile regression\n",
    "#     ax4 = plt.subplot(224)\n",
    "#     change_ratio_retain = np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10)\n",
    "#     change_ratio_forget = np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)\n",
    "    \n",
    "#     # Calculate quantiles instead of moving average\n",
    "#     def rolling_quantiles(data, window=500):\n",
    "#         result_50 = np.zeros_like(data)\n",
    "#         result_25 = np.zeros_like(data)\n",
    "#         result_75 = np.zeros_like(data)\n",
    "        \n",
    "#         for i in range(len(data)):\n",
    "#             start_idx = max(0, i - window//2)\n",
    "#             end_idx = min(len(data), i + window//2)\n",
    "#             window_data = data[start_idx:end_idx]\n",
    "#             result_50[i] = np.percentile(window_data, 50)\n",
    "#             result_25[i] = np.percentile(window_data, 25)\n",
    "#             result_75[i] = np.percentile(window_data, 75)\n",
    "        \n",
    "#         return result_25, result_50, result_75\n",
    "    \n",
    "#     # Calculate and plot quantiles for both retain and forget\n",
    "#     retain_25, retain_50, retain_75 = rolling_quantiles(change_ratio_retain[sort_indices])\n",
    "#     forget_25, forget_50, forget_75 = rolling_quantiles(change_ratio_forget[sort_indices])\n",
    "    \n",
    "#     x = np.arange(len(retain_50))\n",
    "    \n",
    "#     # Plot with confidence bands\n",
    "#     ax4.fill_between(x, retain_25, retain_75, color='blue', alpha=0.2)\n",
    "#     ax4.fill_between(x, forget_25, forget_75, color='red', alpha=0.2)\n",
    "    \n",
    "#     ax4.plot(x, retain_50, color='blue', label='Retain (median)', linewidth=2)\n",
    "#     ax4.plot(x, forget_50, color='red', label='Forget (median)', linewidth=2)\n",
    "    \n",
    "#     ax4.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "#     ax4.set_yscale('log')\n",
    "#     ax4.set_xlabel('Feature Index (sorted by baseline retain activation)')\n",
    "#     ax4.set_ylabel('Change Ratio (finetuned/baseline) (log scale)')\n",
    "#     ax4.set_title('Activation Change Ratios with Confidence Bands')\n",
    "#     ax4.legend()\n",
    "#     ax4.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     return fig\n",
    "\n",
    "# def print_activation_summary(baseline_forget, baseline_retain, \n",
    "#                            finetuned_forget, finetuned_retain):\n",
    "#     \"\"\"\n",
    "#     Calculate and print improved summary statistics for the activation analysis.\n",
    "#     \"\"\"\n",
    "#     # Calculate selectivity metrics\n",
    "#     baseline_selectivity = np.abs(baseline_retain) / (np.abs(baseline_forget) + 1e-10)\n",
    "#     finetuned_selectivity = np.abs(finetuned_retain) / (np.abs(finetuned_forget) + 1e-10)\n",
    "    \n",
    "#     change_ratio_retain = np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10)\n",
    "#     change_ratio_forget = np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)\n",
    "    \n",
    "#     print(\"\\nImproved Activation Analysis Summary:\")\n",
    "#     print(\"\\nSelectivity Statistics (|retain|/|forget|):\")\n",
    "#     print(f\"Baseline - Median: {np.median(baseline_selectivity):.4f}, \"\n",
    "#           f\"25th: {np.percentile(baseline_selectivity, 25):.4f}, \"\n",
    "#           f\"75th: {np.percentile(baseline_selectivity, 75):.4f}\")\n",
    "#     print(f\"Finetuned - Median: {np.median(finetuned_selectivity):.4f}, \"\n",
    "#           f\"25th: {np.percentile(finetuned_selectivity, 25):.4f}, \"\n",
    "#           f\"75th: {np.percentile(finetuned_selectivity, 75):.4f}\")\n",
    "    \n",
    "#     print(f\"\\nChange Ratio Statistics (finetuned/baseline):\")\n",
    "#     print(f\"Retain - Median: {np.median(change_ratio_retain):.4f}, \"\n",
    "#           f\"25th: {np.percentile(change_ratio_retain, 25):.4f}, \"\n",
    "#           f\"75th: {np.percentile(change_ratio_retain, 75):.4f}\")\n",
    "#     print(f\"Forget - Median: {np.median(change_ratio_forget):.4f}, \"\n",
    "#           f\"25th: {np.percentile(change_ratio_forget, 25):.4f}, \"\n",
    "#           f\"75th: {np.percentile(change_ratio_forget, 75):.4f}\")\n",
    "    \n",
    "#     # Calculate improvement metrics with confidence intervals\n",
    "#     def bootstrap_mean(data, n_bootstrap=1000):\n",
    "#         means = np.zeros(n_bootstrap)\n",
    "#         for i in range(n_bootstrap):\n",
    "#             sample = np.random.choice(data, size=len(data), replace=True)\n",
    "#             means[i] = np.mean(sample)\n",
    "#         return np.mean(means), np.percentile(means, [2.5, 97.5])\n",
    "    \n",
    "#     selectivity_improved = finetuned_selectivity > baseline_selectivity\n",
    "#     retain_preserved = change_ratio_retain > 0.9\n",
    "#     forget_reduced = change_ratio_forget < 0.9\n",
    "    \n",
    "#     metrics = {\n",
    "#         'Improved selectivity': selectivity_improved,\n",
    "#         'Preserved retain activation (>90%)': retain_preserved,\n",
    "#         'Reduced forget activation (<90%)': forget_reduced\n",
    "#     }\n",
    "    \n",
    "#     print(\"\\nFeature Improvement Analysis (with 95% confidence intervals):\")\n",
    "#     for metric_name, metric_data in metrics.items():\n",
    "#         mean, (ci_low, ci_high) = bootstrap_mean(metric_data)\n",
    "#         print(f\"{metric_name}: {mean*100:.1f}% [{ci_low*100:.1f}%, {ci_high*100:.1f}%]\")\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main function to load data and generate improved visualizations.\n",
    "#     \"\"\"\n",
    "#     # Define paths\n",
    "#     base_dir = Path(\"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\")\n",
    "#     model_subdir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "    \n",
    "#     # Load baseline data\n",
    "#     baseline_path = base_dir / \"base_model\" / model_subdir\n",
    "#     baseline_forget = np.load(baseline_path / \"activations/feature_acts_forget.npy\")\n",
    "#     baseline_retain = np.load(baseline_path / \"activations/feature_acts_retain.npy\")\n",
    "    \n",
    "#     # Load finetuned data\n",
    "#     finetuned_path = base_dir / \"alpha_0.1\" / model_subdir\n",
    "#     finetuned_forget = np.load(finetuned_path / \"activations/feature_acts_forget.npy\")\n",
    "#     finetuned_retain = np.load(finetuned_path / \"activations/feature_acts_retain.npy\")\n",
    "    \n",
    "#     # Print data shapes and basic statistics\n",
    "#     print(\"Data Analysis:\")\n",
    "#     print(f\"Shape of activation arrays: {baseline_forget.shape}\")\n",
    "#     print(f\"\\nBasic Statistics:\")\n",
    "#     print(f\"{'Dataset':<15} {'Min':>10} {'Max':>10} {'Mean':>10} {'Median':>10}\")\n",
    "#     print(\"-\" * 60)\n",
    "#     for name, data in [\n",
    "#         (\"Baseline Forget\", baseline_forget),\n",
    "#         (\"Baseline Retain\", baseline_retain),\n",
    "#         (\"Finetuned Forget\", finetuned_forget),\n",
    "#         (\"Finetuned Retain\", finetuned_retain)\n",
    "#     ]:\n",
    "#         print(f\"{name:<15} {np.min(data):10.2e} {np.max(data):10.2e} \"\n",
    "#               f\"{np.mean(data):10.2e} {np.median(data):10.2e}\")\n",
    "    \n",
    "#     # Generate improved visualizations\n",
    "#     fig = plot_improved_activations(baseline_forget, baseline_retain,\n",
    "#                                   finetuned_forget, finetuned_retain)\n",
    "    \n",
    "#     # Save the figure with high resolution\n",
    "#     plt.savefig('improved_activation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "    \n",
    "#     # Print detailed summary statistics\n",
    "#     print_activation_summary(baseline_forget, baseline_retain,\n",
    "#                            finetuned_forget, finetuned_retain)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c82a5d6d-c734-41a6-bb38-48651cbabba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "\n",
      "Median Activation Magnitudes:\n",
      "Baseline Retain: 7.2640e-03\n",
      "Finetuned Retain: 2.6610e-05\n",
      "Baseline Forget: 5.9969e-03\n",
      "Finetuned Forget: 7.1051e-05\n",
      "\n",
      "Median Suppression Ratios (Finetuned/Baseline):\n",
      "Retain: 0.0020\n",
      "Forget: 0.0158\n",
      "\n",
      "Percentage of features with stronger forget suppression: 26.4%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_distributions(baseline_forget, baseline_retain, \n",
    "                      finetuned_forget, finetuned_retain,\n",
    "                      save_path='distribution_plot.png'):\n",
    "    \"\"\"\n",
    "    Create violin plots showing the distribution of activations before and after finetuning.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare data for violin plots\n",
    "    data_dict = {\n",
    "        'Baseline Retain': np.abs(baseline_retain),\n",
    "        'Finetuned Retain': np.abs(finetuned_retain),\n",
    "        'Baseline Forget': np.abs(baseline_forget),\n",
    "        'Finetuned Forget': np.abs(finetuned_forget)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Activation': np.concatenate([v for v in data_dict.values()]),\n",
    "        'Type': np.concatenate([[k] * len(v) for k, v in data_dict.items()])\n",
    "    })\n",
    "    \n",
    "    # Create violin plot\n",
    "    sns.violinplot(data=df, x='Type', y='Activation', cut=0)\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Activation Magnitude (log scale)')\n",
    "    plt.title('Distribution of Activation Magnitudes')\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_comparison(baseline_forget, baseline_retain, \n",
    "                          finetuned_forget, finetuned_retain,\n",
    "                          save_path='feature_comparison.png'):\n",
    "    \"\"\"\n",
    "    Create scatter plot comparing retain vs forget suppression ratios.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Calculate ratios (adding small epsilon to prevent division by zero)\n",
    "    retain_ratio = np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10)\n",
    "    forget_ratio = np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.scatter(retain_ratio, forget_ratio, alpha=0.1, s=1)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    max_val = max(retain_ratio.max(), forget_ratio.max())\n",
    "    min_val = min(retain_ratio.min(), forget_ratio.min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Equal suppression')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Retain Activation Ratio (Finetuned/Baseline)')\n",
    "    plt.ylabel('Forget Activation Ratio (Finetuned/Baseline)')\n",
    "    plt.title('Feature-wise Comparison of Activation Changes')\n",
    "    \n",
    "    # Add annotations\n",
    "    plt.text(0.05, 0.95, 'Stronger forget suppression', \n",
    "             transform=plt.gca().transAxes, \n",
    "             verticalalignment='top')\n",
    "    plt.text(0.95, 0.05, 'Stronger retain suppression', \n",
    "             transform=plt.gca().transAxes, \n",
    "             horizontalalignment='right')\n",
    "    \n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_magnitude_analysis(baseline_forget, baseline_retain, \n",
    "                          finetuned_forget, finetuned_retain,\n",
    "                          save_path='magnitude_analysis.png'):\n",
    "    \"\"\"\n",
    "    Create plot showing activation magnitudes sorted by baseline retain activation.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Sort all arrays by baseline retain activation\n",
    "    sort_indices = np.argsort(-(baseline_retain))\n",
    "    x = np.arange(len(baseline_retain))\n",
    "    \n",
    "    # Function to compute rolling statistics\n",
    "    def rolling_stats(data, window=500):\n",
    "        result_median = np.zeros_like(data, dtype=float)\n",
    "        result_25 = np.zeros_like(data, dtype=float)\n",
    "        result_75 = np.zeros_like(data, dtype=float)\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            start_idx = max(0, i - window//2)\n",
    "            end_idx = min(len(data), i + window//2)\n",
    "            window_data = data[start_idx:end_idx]\n",
    "            result_median[i] = np.median(window_data)\n",
    "            result_25[i] = np.percentile(window_data, 25)\n",
    "            result_75[i] = np.percentile(window_data, 75)\n",
    "        \n",
    "        return result_25, result_median, result_75\n",
    "\n",
    "    # Compute rolling statistics for each series\n",
    "    br_25, br_med, br_75 = rolling_stats(np.abs(baseline_retain[sort_indices]))\n",
    "    fr_25, fr_med, fr_75 = rolling_stats(np.abs(finetuned_retain[sort_indices]))\n",
    "    bf_25, bf_med, bf_75 = rolling_stats(np.abs(baseline_forget[sort_indices]))\n",
    "    ff_25, ff_med, ff_75 = rolling_stats(np.abs(finetuned_forget[sort_indices]))\n",
    "\n",
    "    # Plot with shaded regions for IQR\n",
    "    plt.fill_between(x, br_25, br_75, alpha=0.2, color='darkblue')\n",
    "    plt.fill_between(x, fr_25, fr_75, alpha=0.2, color='blue')\n",
    "    plt.fill_between(x, bf_25, bf_75, alpha=0.2, color='darkred')\n",
    "    plt.fill_between(x, ff_25, ff_75, alpha=0.2, color='red')\n",
    "\n",
    "    # Plot median lines\n",
    "    plt.plot(x, br_med, color='darkblue', label='Baseline Retain', linewidth=2)\n",
    "    plt.plot(x, fr_med, color='blue', label='Finetuned Retain', linewidth=2)\n",
    "    plt.plot(x, bf_med, color='darkred', label='Baseline Forget', linewidth=2)\n",
    "    plt.plot(x, ff_med, color='red', label='Finetuned Forget', linewidth=2)\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Feature Index (sorted by baseline retain activation)')\n",
    "    plt.ylabel('Activation Magnitude (log scale)')\n",
    "    plt.title('Activation Magnitudes Across Features')\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data and generate all visualizations.\n",
    "    \"\"\"\n",
    "    # Define paths\n",
    "    base_dir = Path(\"/data/aashiq_muhamed/unlearning/SAEBench/eval_results/activation_mcq\")\n",
    "    model_subdir = \"sae_bench_gemma-2-2b_topk_width-2pow14_date-1109_blocks.5.hook_resid_post__trainer_2/gemma-2-2b-it/results\"\n",
    "    \n",
    "    # Load baseline data\n",
    "    baseline_path = base_dir / \"base_model\" / model_subdir\n",
    "    baseline_forget = np.load(baseline_path / \"activations/feature_acts_forget.npy\")\n",
    "    baseline_retain = np.load(baseline_path / \"activations/feature_acts_retain.npy\")\n",
    "    \n",
    "    # Load finetuned data\n",
    "    finetuned_path = base_dir / \"alpha_0.1\" / model_subdir\n",
    "    finetuned_forget = np.load(finetuned_path / \"activations/feature_acts_forget.npy\")\n",
    "    finetuned_retain = np.load(finetuned_path / \"activations/feature_acts_retain.npy\")\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    plot_distributions(baseline_forget, baseline_retain,\n",
    "                      finetuned_forget, finetuned_retain,\n",
    "                      'activation_distributions.png')\n",
    "    \n",
    "    plot_feature_comparison(baseline_forget, baseline_retain,\n",
    "                          finetuned_forget, finetuned_retain,\n",
    "                          'feature_comparison.png')\n",
    "    \n",
    "    plot_magnitude_analysis(baseline_forget, baseline_retain,\n",
    "                          finetuned_forget, finetuned_retain,\n",
    "                          'magnitude_analysis.png')\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"\\nMedian Activation Magnitudes:\")\n",
    "    print(f\"Baseline Retain: {np.median(np.abs(baseline_retain)):.4e}\")\n",
    "    print(f\"Finetuned Retain: {np.median(np.abs(finetuned_retain)):.4e}\")\n",
    "    print(f\"Baseline Forget: {np.median(np.abs(baseline_forget)):.4e}\")\n",
    "    print(f\"Finetuned Forget: {np.median(np.abs(finetuned_forget)):.4e}\")\n",
    "    \n",
    "    # Calculate suppression ratios\n",
    "    retain_ratio = np.median(np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10))\n",
    "    forget_ratio = np.median(np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10))\n",
    "    \n",
    "    print(\"\\nMedian Suppression Ratios (Finetuned/Baseline):\")\n",
    "    print(f\"Retain: {retain_ratio:.4f}\")\n",
    "    print(f\"Forget: {forget_ratio:.4f}\")\n",
    "    \n",
    "    # Calculate percentage of features with stronger forget suppression\n",
    "    stronger_forget = np.mean(\n",
    "        (np.abs(finetuned_forget) / (np.abs(baseline_forget) + 1e-10)) <\n",
    "        (np.abs(finetuned_retain) / (np.abs(baseline_retain) + 1e-10))\n",
    "    ) * 100\n",
    "    \n",
    "    print(f\"\\nPercentage of features with stronger forget suppression: {stronger_forget:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4220c6e-e0ca-4f83-a3a2-bf79bfa1cb27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
